def word_tokenize(pattern):
    return pattern.split(' ')